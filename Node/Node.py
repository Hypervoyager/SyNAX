import copy
# from torch.cuda import random
import random
import numpy as np

from re import S

import torch
import torch.nn as nn
from numpy import s_

import models.Model as Model
from models.builder import build_model
from models.builder_snn import build_snn_model
import utils.trans_utils
from utils.MeZO import MeZO
from utils.utils import ann2snn, get_params_need_grad, compute_snn_difference
from utils.trans_utils import SOPMonitor, reset_net, accuracy

from timm.models import create_model
import models.model_eva
import models.model_vit


def init_model(model_type):
    model = []
    if model_type == 'LeNet5':
        model = Model.LeNet5()
    elif model_type == 'MLP':
        model = Model.MLP()
    elif model_type == 'ResNet18':
        model = Model.ResNet18()
    elif model_type == 'CNN':
        model = Model.CNN()
    return model


def init_optimizer(model, args):
    optimizer = []
    if args.optimizer == 'sgd':
        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=5e-4)
    elif args.optimizer == 'adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-2)
    elif args.optimizer == 'adamw':
        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.999), weight_decay=0.05)
    return optimizer





class Node(object):
    def __init__(self, num, train_loader, test_data, args, device_type="zero"):
        self.args = args
        self.num = num + 1
        self.device = self.args.device
        self.train_data = train_loader
        self.device_type = device_type
        self.test_data = test_data
        self.loss = torch.nn.CrossEntropyLoss(reduction='mean')
        self.zero_epsilon = args.zo_epsilon
        self.scaling_factor = None


    def ann_fork(self, global_node):
        self.model = copy.deepcopy(global_node.model).to(self.device)
        self.optimizer = init_optimizer(self.model, self.args)


    def snn_fork(self, global_node, zero_direct):
        self.model = copy.deepcopy(global_node.snn_model).to(self.device)


    def fork(self, global_node):
        self.model = copy.deepcopy(global_node.model).to(self.device)
        self.optimizer = init_optimizer(self.model, self.args)
    

    def get_perturbed_model(self, zero_direct, sign):
        """ è·å–æ‰°åŠ¨åçš„ SNN æ¨¡å‹ """
        perturbed_model = copy.deepcopy(self.model)  # å¤åˆ¶æ¨¡å‹ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ¨¡å‹
        for name, param in perturbed_model.named_parameters():
            if name in zero_direct:
                param.data += sign * self.zero_epsilon * zero_direct[name]  # ç›´æ¥ä¿®æ”¹æ¨¡å‹å‚æ•°
        return perturbed_model  # è¿”å›çš„æ˜¯ä¸€ä¸ª PyTorch æ¨¡å‹


    def zero_order_update(self, loss_plus, loss_minus, zero_direct, zo_lr=0.01):
        """ é€šè¿‡é›¶é˜¶ä¼˜åŒ–æ›´æ–° SNN æ¨¡å‹ """
        # ç¡®ä¿ loss ä¸æ˜¯ None
        diff = loss_plus - loss_minus
        status = "æœ‰å®³çš„" if diff > 0 else "æœ‰ç›Šçš„"
        print(f"loss_plus: {loss_plus:.4f}, loss_minus: {loss_minus:.4f}, diff: {diff:.4f} -> æ­¤æ¬¡å‚è€ƒæ¢¯åº¦ä¸º{status}")

        if loss_plus is None or loss_minus is None:
            print("ğŸš¨ Error: loss_plus or loss_minus is None!")
            return  # é¿å…é”™è¯¯

        self.scaling_factor = -((loss_plus - loss_minus) / (2 * self.zero_epsilon))



    def evaluate(self):
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        test_loader = self.test_data  # ç¡®ä¿ test_loader æ­£ç¡®
        correct = 0
        total = 0
        loss_fn = nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•°
        total_loss = 0.0

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)  # å‰å‘ä¼ æ’­
                loss = loss_fn(output, target)  # è®¡ç®— loss
                total_loss += loss.item() * data.size(0)

                pred = output.argmax(dim=1)  # è·å–é¢„æµ‹ç±»åˆ«
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

        acc = 100.0 * correct / total  # è®¡ç®—å‡†ç¡®ç‡
        avg_loss = total_loss / total  # è®¡ç®—å¹³å‡ loss

        print(f"ğŸŒŸ æµ‹è¯•ç»“æœ: Loss={avg_loss:.4f}, Accuracy={acc:.2f}%")
        return avg_loss, acc  # è¿”å› loss å’Œ å‡†ç¡®ç‡


    def delete_model(self):
        # åˆ é™¤æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'optimizer'):
            del self.optimizer 
        torch.cuda.empty_cache()  # æ¸…ç©ºæœªä½¿ç”¨çš„æ˜¾å­˜ç¼“å­˜

    def adjust(self):
        self.model = copy.deepcopy(self.model).to(self.device)
        self.optimizer = init_optimizer(self.model, self.args)




class Select_Node(object):
    def __init__(self, args):
        self.args = args
        self.s_list = []   
        self.c_list = []   
        self.node_list = list(range(args.node_num))
        self.max_lost = args.max_lost   

        for j in range(self.max_lost):
            self.s_list.extend(self.node_list)
    

    def random_select(self):
        index = random.randrange(len(self.s_list))      
        chosen_number = self.s_list.pop(index)          
        self.c_list.append(chosen_number)               
        print(self.c_list)

        if len(set(self.c_list)) == self.args.node_num :
            self.s_list.extend(self.node_list)          
            [self.c_list.remove(i) for i in range(self.args.node_num)]     
        return chosen_number


class Global_Node(object):
    def __init__(self, test_data, args):
        self.num = 0
        self.args = args
        self.device = self.args.device
        self.model = build_model(args, args.model).to(args.device)
        self.test_data = test_data  
        self.accumulated_state_dict = None  # ç´¯åŠ å­—å…¸ï¼Œåˆå§‹ä¸º None
        self.merged_nodes = 0  # ç”¨äºè®°å½•å·²å‚ä¸åˆå¹¶çš„èŠ‚ç‚¹æ•°
        self.ann_gradient = None  # è®°å½• ANN å…¨å±€æ¢¯åº¦
        self.snn_gradient = None  # è®°å½• SNN å…¨å±€æ¢¯åº¦
        # self.snn_model = build_snn_model(args).to(args.device)
        self.snn_model = ann2snn(self.model, self.test_data, args = self.args)
        self.snn_old_model = copy.deepcopy(self.snn_model)
        # åŠ¨é‡å­˜å‚¨
        self.global_grad_momentum = None  # å†å²å…¨å±€æ¢¯åº¦
        self.ann_grad_momentum = None  # å†å² ANN æ¢¯åº¦
        self.snn_grad_momentum = None  # å†å² SNN æ¢¯åº¦
        self.zero_direct = None
        self.zero_momentum = {}

        
        self.Dict = self.model.state_dict()

        # self.edge_node = [build_model(args, args.local_model).to(args.device) for k in range(args.node_num)]
        self.init = False
        self.save = []


    def merge_init(self):
        """
        åˆå§‹åŒ–æˆ–é‡ç½®æ¨¡å‹çš„ç´¯åŠ å­—å…¸å’Œè®¡æ•°å™¨ã€‚
        """
        # è·å–å½“å‰æ¨¡å‹çš„å‚æ•°å­—å…¸
        state_dict = self.model.state_dict()

        # åˆå§‹åŒ–ç´¯åŠ å­—å…¸ä¸ºå…¨é›¶
        self.accumulated_state_dict = {key: torch.zeros_like(value) for key, value in state_dict.items()}
        
        # é‡ç½®å·²åˆå¹¶èŠ‚ç‚¹è®¡æ•°å™¨
        self.merged_nodes = 0

        # åˆå§‹åŒ–åŠ¨é‡å­˜å‚¨
        if self.global_grad_momentum is None:
            self.global_grad_momentum = {key: torch.zeros_like(value) for key, value in state_dict.items()}
        if self.ann_grad_momentum is None:
            self.ann_grad_momentum = {key: torch.zeros_like(value) for key, value in state_dict.items()}
        if self.snn_grad_momentum is None:
            self.snn_grad_momentum = {key: torch.zeros_like(value) for key, value in state_dict.items()}

        # åˆå§‹åŒ– ANN å’Œ SNN æ¢¯åº¦å­˜å‚¨å­—å…¸
        self.ann_gradient = {key: torch.zeros_like(value) for key, value in state_dict.items()}
        self.snn_gradient = {key: torch.zeros_like(value) for key, value in state_dict.items()}

        # åˆå§‹åŒ–åŠ¨é‡å­˜å‚¨
        if self.global_grad_momentum is None:
            self.global_grad_momentum = {key: torch.zeros_like(value) for key, value in state_dict.items()}

        print("Global model merge initialized.")


    def update_momentum(self, current_gradient, momentum_dict, beta=0.9):
        """
        æ›´æ–°åŠ¨é‡å˜é‡ã€‚

        Args:
            current_gradient: å½“å‰çš„æ¢¯åº¦å­—å…¸ã€‚
            momentum_dict: å¯¹åº”çš„åŠ¨é‡å­—å…¸ï¼ˆglobal_grad_momentum, ann_grad_momentum, snn_grad_momentumï¼‰ã€‚
            beta: åŠ¨é‡ç³»æ•°ã€‚
        """
        for key in current_gradient.keys():
            momentum_dict[key] = beta * momentum_dict[key] + (1 - beta) * current_gradient[key]

    def merge_now(self, Edge_node, device_type):
        """
        åˆå¹¶å®¢æˆ·ç«¯æ¨¡å‹åˆ°å…¨å±€æ¨¡å‹ï¼Œå¹¶è®¡ç®—æ¢¯åº¦ã€‚

        Args:
            Edge_node: å½“å‰å®¢æˆ·ç«¯èŠ‚ç‚¹ï¼ŒåŒ…å«å…¶æ¨¡å‹å’Œç¼–å·ã€‚
            device_type: å®¢æˆ·ç«¯è®¾å¤‡ç±»å‹ï¼ˆ"ANN" æˆ– "SNN"ï¼‰ã€‚
        """
        Edge_node_State_List = Edge_node.model.state_dict()
        Global_node_State_List = self.model.state_dict()

        # è®¡ç®—æ¢¯åº¦ï¼šå®¢æˆ·ç«¯æ¨¡å‹å‚æ•° - å…¨å±€æ¨¡å‹å‚æ•°
        gradient = {key: Edge_node_State_List[key].float() - Global_node_State_List[key].float()
                    for key in Global_node_State_List.keys()}

        # ç´¯åŠ å½“å‰å®¢æˆ·ç«¯æ¨¡å‹å‚æ•°ï¼ˆç”¨äºå…¨å±€æ¨¡å‹æ›´æ–°ï¼‰
        for key in self.accumulated_state_dict.keys():
            self.accumulated_state_dict[key] += Edge_node_State_List[key].float()

        # ç´¯åŠ æ¢¯åº¦åˆ°å¯¹åº”è®¾å¤‡ç±»å‹çš„æ¢¯åº¦å­˜å‚¨å­—å…¸
        if device_type == "ANN":
            for key in self.ann_gradient.keys():
                self.ann_gradient[key] += gradient[key]
        elif device_type == "SNN":
            for key in self.snn_gradient.keys():
                self.snn_gradient[key] += gradient[key]

        # æ›´æ–°å·²åˆå¹¶çš„èŠ‚ç‚¹æ•°
        self.merged_nodes += 1


    def aggregate_snn(self, avg_scaling_factor):
        global_state_dict  = self.model.state_dict()
        
        # è®¡ç®—æ›´æ–°åçš„å‚æ•°
        for key in global_state_dict.keys():
            if key in self.ann_gradient:  # ç¡®ä¿æ¢¯åº¦å­—å…¸é‡Œæœ‰è¿™ä¸ªå‚æ•°
                global_state_dict[key] += avg_scaling_factor * self.ann_gradient[key]

        # æ›´æ–°å…¨å±€æ¨¡å‹
        self.model.load_state_dict(global_state_dict)


    def finish_merge_momentum(self, num_nodes, beta=0.2):
        """
        å®Œæˆåˆå¹¶, è®¡ç®—å‚æ•°å‡å€¼å’Œå¸¦åŠ¨é‡çš„æ‰°åŠ¨(zero_direct)ã€‚

        Args:
            num_nodes: å½“å‰å‚ä¸èšåˆçš„è®¾å¤‡æ•°é‡
            beta: åŠ¨é‡ç³»æ•°(0~1),è¶Šå¤§ä¿ç•™æ—§åŠ¨é‡è¶Šå¤š

        Returns:
            zero_direct: ç”¨åŠ¨é‡å¹³æ»‘åçš„æ‰°åŠ¨å‘é‡
            norm: å½“å‰æ‰°åŠ¨çš„èŒƒæ•°ï¼ˆæœªåŠ¨é‡å¹³æ»‘å‰ï¼‰
        """
        # 1ï¸âƒ£ èšåˆ ANN å‚æ•°
        for key in self.accumulated_state_dict.keys():
            self.accumulated_state_dict[key] /= num_nodes

        for key in self.ann_gradient.keys():
            self.ann_gradient[key] /= num_nodes

        # 2ï¸âƒ£ ä¿å­˜æ—§çš„ SNN æ¨¡å‹
        self.snn_old_model.load_state_dict(self.snn_model.state_dict(), strict=False)

        # 3ï¸âƒ£ æ›´æ–°å…¨å±€ ANN æ¨¡å‹
        self.model.load_state_dict(self.accumulated_state_dict)

        # 4ï¸âƒ£ å°† ANN è½¬æ¢ä¸ºæ–°çš„ SNN æ¨¡å‹
        self.snn_model = ann2snn(self.model, self.test_data, args=self.args)

        # 5ï¸âƒ£ è®¡ç®—æ‰°åŠ¨å‘é‡ï¼ˆå½“å‰å›åˆ SNN å‚æ•°å˜åŒ–ï¼‰
        delta_params, norm = compute_snn_difference(self.snn_model, self.snn_old_model)

        # 6ï¸âƒ£ ä½¿ç”¨åŠ¨é‡æ›´æ–° zero_direct
        zero_direct = {}
        for name in self.snn_model.state_dict().keys():
            if name in delta_params:
                new_delta = delta_params[name]

                # å¦‚æœä¹‹å‰æ²¡æœ‰åŠ¨é‡ï¼Œåˆå§‹åŒ–ä¸ºå½“å‰å€¼
                if name not in self.zero_momentum:
                    self.zero_momentum[name] = new_delta.clone()
                else:
                    # åº”ç”¨åŠ¨é‡å…¬å¼ï¼šv_t = Î² * v_{t-1} + (1 - Î²) * g_t
                    self.zero_momentum[name] = beta * self.zero_momentum[name] +  new_delta

                zero_direct[name] = self.zero_momentum[name].clone()

        print(f"ğŸ¯ Zero-direct åŠ¨é‡èŒƒæ•° (å½“å‰æ‰°åŠ¨ norm): {norm:.4f}")
        return zero_direct, norm


    def finish_merge(self, num_nodes, device_type, beta=0.9):
        """
        å®Œæˆåˆå¹¶ï¼Œè®¡ç®—å‚æ•°å‡å€¼å’Œæ¢¯åº¦å‡å€¼ã€‚

        Args:
            num_nodes: è®¾å¤‡æ•°é‡ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰ã€‚
            device_type: å®¢æˆ·ç«¯è®¾å¤‡ç±»å‹ï¼ˆ"ANN" æˆ– "SNN"ï¼‰ã€‚

        Returns:
            å¹³å‡æ¢¯åº¦å­—å…¸ã€‚
        """
        # è®¡ç®—å…¨å±€æ¨¡å‹çš„å¹³å‡å‚æ•°
        for key in self.accumulated_state_dict.keys():
            self.accumulated_state_dict[key] /= num_nodes

        for key in self.ann_gradient.keys():
            self.ann_gradient[key] /= num_nodes  # 
        # å°†åŸæ¨¡å‹è½¬æ¢ä¸ºSNN
        self.snn_old_model.load_state_dict(self.snn_model.state_dict(), strict=False)
        # å°†å¹³å‡å‚æ•°æ›´æ–°åˆ°å…¨å±€æ¨¡å‹
        self.model.load_state_dict(self.accumulated_state_dict)

        # å°†æ–°çš„å…¨å±€æ¨¡å‹è½¬æ¢ä¸ºSNN
        self.snn_model = ann2snn(self.model, self.test_data, args = self.args)

        # è®¡ç®—æœ‰æ¢¯åº¦çš„å‚æ•°çš„å·®å€¼å¹¶å½’ä¸€åŒ–
        delta_params, norm = compute_snn_difference(self.snn_model, self.snn_old_model)

        print('norm:', norm)
        #  è®© zero_direct å˜æˆå­—å…¸ï¼Œè€Œä¸æ˜¯åˆ—è¡¨
        zero_direct = {name: delta_params[name].clone() for name in self.snn_model.state_dict().keys() if name in delta_params}

        # self.zero_direct = 

        return zero_direct, norm
    
    def finish_merge_ann(self, num_nodes, device_type, beta=0.9):
        """
        å®Œæˆåˆå¹¶ï¼Œè®¡ç®—å‚æ•°å‡å€¼å’Œæ¢¯åº¦å‡å€¼ã€‚

        Args:
            num_nodes: è®¾å¤‡æ•°é‡ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰ã€‚
            device_type: å®¢æˆ·ç«¯è®¾å¤‡ç±»å‹ï¼ˆ"ANN" æˆ– "SNN"ï¼‰ã€‚

        Returns:
            å¹³å‡æ¢¯åº¦å­—å…¸ã€‚
        """
        # è®¡ç®—å…¨å±€æ¨¡å‹çš„å¹³å‡å‚æ•°
        for key in self.accumulated_state_dict.keys():
            self.accumulated_state_dict[key] /= num_nodes

        for key in self.ann_gradient.keys():
            self.ann_gradient[key] /= num_nodes  # 


        self.model.load_state_dict(self.accumulated_state_dict)
    
    def evaluate(self):
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        test_loader = self.test_data  # ç¡®ä¿ test_loader æ­£ç¡®
        correct = 0
        total = 0
        loss_fn = nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•°
        total_loss = 0.0

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)[0]  # å‰å‘ä¼ æ’­
                loss = loss_fn(output, target)  # è®¡ç®— loss
                total_loss += loss.item() * data.size(0)

                pred = output.argmax(dim=1)  # è·å–é¢„æµ‹ç±»åˆ«
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

        acc = 100.0 * correct / total  # è®¡ç®—å‡†ç¡®ç‡
        avg_loss = total_loss / total  # è®¡ç®—å¹³å‡ loss

        print(f"ğŸŒŸ æµ‹è¯•ç»“æœ: Loss={avg_loss:.4f}, Accuracy={acc:.2f}%")
        return avg_loss, acc  # è¿”å› loss å’Œ å‡†ç¡®ç‡
        
    def get_normalized_global_gradient(self, epsilon=1e-3):
        """
        è·å–å½’ä¸€åŒ–çš„å…¨å±€æ¢¯åº¦ï¼Œå¹¶ä¹˜ä»¥æ‰°åŠ¨å¹…åº¦ç³»æ•° epsilonã€‚

        Args:
            epsilon: æ‰°åŠ¨å¹…åº¦ç³»æ•°ã€‚

        Returns:
            å½’ä¸€åŒ–åçš„å…¨å±€æ¢¯åº¦å­—å…¸ã€‚
        """
        if self.global_grad_momentum is None:
            raise ValueError("Global gradient momentum is not initialized. Please call merge_init first.")

        normalized_gradient = {}
        for key, value in self.global_grad_momentum.items():
            grad_norm = torch.norm(value)  # è®¡ç®—æ¢¯åº¦çš„ L2 èŒƒæ•°
            if grad_norm > 0:  # é¿å…é™¤ä»¥é›¶
                normalized_gradient[key] = epsilon * (value / grad_norm)
            else:
                normalized_gradient[key] = torch.zeros_like(value)  # å¦‚æœæ¢¯åº¦å…¨ä¸ºé›¶ï¼Œä¿æŒä¸ºé›¶

        return normalized_gradient



    def evaluate_snn(self, T=8):
        self.snn_model.to(self.device)
        self.snn_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        test_loader = self.test_data  # ç¡®ä¿ test_loader æ­£ç¡®
        correct = 0
        total = 0
        loss_fn = nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•°
        total_loss = 0.0

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                # åœ¨ T ä¸ªæ—¶é—´æ­¥ä¸Šç´¯ç§¯è¾“å‡º
                accumulated_output = torch.zeros((data.shape[0], self.snn_model.num_classes), device=self.device)
                
                for t in range(T):
                    output = self.snn_model(data)[0]  # å‰å‘ä¼ æ’­
                    accumulated_output += output  # ç´¯ç§¯è¾“å‡º
                
                averaged_output = accumulated_output / T  # è®¡ç®—å¹³å‡è¾“å‡º

                loss = loss_fn(averaged_output, target)  # è®¡ç®— loss
                total_loss += loss.item() * data.size(0)

                pred = averaged_output.argmax(dim=1)  # è·å–æœ€ç»ˆé¢„æµ‹ç±»åˆ«
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)

        acc = 100.0 * correct / total  # è®¡ç®—å‡†ç¡®ç‡
        avg_loss = total_loss / total  # è®¡ç®—å¹³å‡ loss

        print(f"ğŸŒŸ æµ‹è¯•ç»“æœ: Loss={avg_loss:.4f}, Accuracy={acc:.2f}%")
        return avg_loss, acc  # è¿”å› loss å’Œ å‡†ç¡®ç‡
    


    


